{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>I’m pregnant. It will be scary, but I’m ready ...</td>\n",
       "      <td>Confident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Please join me in sending prayers, strength an...</td>\n",
       "      <td>Sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>the feeling or belief that one can rely on som...</td>\n",
       "      <td>Confident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>When we feel confident in others, we know and ...</td>\n",
       "      <td>Confident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>When we feel confident in ourselves, we are ab...</td>\n",
       "      <td>Confident</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text      Class\n",
       "0  I’m pregnant. It will be scary, but I’m ready ...  Confident\n",
       "1  Please join me in sending prayers, strength an...        Sad\n",
       "2  the feeling or belief that one can rely on som...  Confident\n",
       "3  When we feel confident in others, we know and ...  Confident\n",
       "4  When we feel confident in ourselves, we are ab...  Confident"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing dataset\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data = data.drop(columns=[\"Timestamp\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing text\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "punct = string.punctuation + \"’\"\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    no_punct = \"\".join([c for c in text if c not in punct])\n",
    "    return no_punct.lower()\n",
    "\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda x:\" \".join(x))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(data['Text'])\n",
    "\n",
    "X = vectorizer.transform(data['Text'])\n",
    "Y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing the data in validation and train set for evaluation\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Analytical       0.43      0.82      0.56        11\n",
      "    Confident       0.90      1.00      0.95         9\n",
      "Disrespectful       0.57      0.80      0.67        10\n",
      "     Doubtful       0.00      0.00      0.00        10\n",
      "     Friendly       0.00      0.00      0.00         5\n",
      "      Hostile       0.67      0.50      0.57        12\n",
      "       Joyful       0.67      1.00      0.80         6\n",
      "   Optimistic       0.40      0.67      0.50         9\n",
      "  Pessimistic       0.33      0.25      0.29         8\n",
      "   Respectful       1.00      0.29      0.44         7\n",
      "          Sad       0.77      1.00      0.87        10\n",
      "       Urgent       0.67      0.40      0.50         5\n",
      "\n",
      "     accuracy                           0.59       102\n",
      "    macro avg       0.53      0.56      0.51       102\n",
      " weighted avg       0.54      0.59      0.53       102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(solver='lbfgs',multi_class='auto')\n",
    "lr_model.fit(train_x,train_y)\n",
    "\n",
    "lr_pred = lr_model.predict(test_x)\n",
    "print(classification_report(test_y, lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Analytical       0.64      0.82      0.72        11\n",
      "    Confident       0.69      1.00      0.82         9\n",
      "Disrespectful       1.00      0.70      0.82        10\n",
      "     Doubtful       0.00      0.00      0.00        10\n",
      "     Friendly       0.00      0.00      0.00         5\n",
      "      Hostile       0.70      0.58      0.64        12\n",
      "       Joyful       0.22      1.00      0.36         6\n",
      "   Optimistic       0.31      0.44      0.36         9\n",
      "  Pessimistic       0.25      0.12      0.17         8\n",
      "   Respectful       1.00      0.14      0.25         7\n",
      "          Sad       0.83      1.00      0.91        10\n",
      "       Urgent       1.00      0.20      0.33         5\n",
      "\n",
      "     accuracy                           0.54       102\n",
      "    macro avg       0.55      0.50      0.45       102\n",
      " weighted avg       0.57      0.54      0.49       102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Naïve Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(train_x,train_y)\n",
    "\n",
    "mnb_pred = mnb_model.predict(test_x)\n",
    "print(classification_report(test_y, mnb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Analytical       0.83      0.91      0.87        11\n",
      "    Confident       0.90      1.00      0.95         9\n",
      "Disrespectful       0.58      0.70      0.64        10\n",
      "     Doubtful       0.64      0.70      0.67        10\n",
      "     Friendly       0.75      0.60      0.67         5\n",
      "      Hostile       0.78      0.58      0.67        12\n",
      "       Joyful       0.56      0.83      0.67         6\n",
      "   Optimistic       0.43      0.33      0.38         9\n",
      "  Pessimistic       0.25      0.25      0.25         8\n",
      "   Respectful       1.00      0.43      0.60         7\n",
      "          Sad       0.77      1.00      0.87        10\n",
      "       Urgent       0.75      0.60      0.67         5\n",
      "\n",
      "     accuracy                           0.68       102\n",
      "    macro avg       0.69      0.66      0.66       102\n",
      " weighted avg       0.69      0.68      0.67       102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_model = SGDClassifier()\n",
    "sgd_model.fit(train_x,train_y)\n",
    "\n",
    "sgd_pred = sgd_model.predict(test_x)\n",
    "print(classification_report(test_y, sgd_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Analytical       0.60      0.82      0.69        11\n",
      "    Confident       0.42      0.89      0.57         9\n",
      "Disrespectful       0.67      0.60      0.63        10\n",
      "     Doubtful       0.67      0.20      0.31        10\n",
      "     Friendly       1.00      0.40      0.57         5\n",
      "      Hostile       0.70      0.58      0.64        12\n",
      "       Joyful       0.55      1.00      0.71         6\n",
      "   Optimistic       0.30      0.33      0.32         9\n",
      "  Pessimistic       0.17      0.12      0.14         8\n",
      "   Respectful       1.00      0.14      0.25         7\n",
      "          Sad       0.89      0.80      0.84        10\n",
      "       Urgent       0.43      0.60      0.50         5\n",
      "\n",
      "     accuracy                           0.55       102\n",
      "    macro avg       0.62      0.54      0.51       102\n",
      " weighted avg       0.61      0.55      0.52       102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbours\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(train_x,train_y)\n",
    "\n",
    "knn_pred = knn_model.predict(test_x)\n",
    "print(classification_report(test_y, knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Analytical       0.67      0.36      0.47        11\n",
      "    Confident       0.78      0.78      0.78         9\n",
      "Disrespectful       0.28      0.90      0.43        10\n",
      "     Doubtful       1.00      0.60      0.75        10\n",
      "     Friendly       0.14      0.20      0.17         5\n",
      "      Hostile       0.60      0.50      0.55        12\n",
      "       Joyful       0.56      0.83      0.67         6\n",
      "   Optimistic       0.40      0.22      0.29         9\n",
      "  Pessimistic       0.25      0.25      0.25         8\n",
      "   Respectful       0.50      0.29      0.36         7\n",
      "          Sad       0.50      0.20      0.29        10\n",
      "       Urgent       1.00      0.40      0.57         5\n",
      "\n",
      "     accuracy                           0.47       102\n",
      "    macro avg       0.56      0.46      0.46       102\n",
      " weighted avg       0.56      0.47      0.47       102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(train_x,train_y)\n",
    "\n",
    "dt_pred = dt_model.predict(test_x)\n",
    "print(classification_report(test_y, dt_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Analytical       1.00      0.64      0.78        11\n",
      "    Confident       0.53      1.00      0.69         9\n",
      "Disrespectful       0.27      0.70      0.39        10\n",
      "     Doubtful       0.67      0.40      0.50        10\n",
      "     Friendly       0.25      0.20      0.22         5\n",
      "      Hostile       0.78      0.58      0.67        12\n",
      "       Joyful       0.67      0.67      0.67         6\n",
      "   Optimistic       0.57      0.44      0.50         9\n",
      "  Pessimistic       0.20      0.12      0.15         8\n",
      "   Respectful       1.00      0.43      0.60         7\n",
      "          Sad       0.67      0.60      0.63        10\n",
      "       Urgent       0.67      0.40      0.50         5\n",
      "\n",
      "     accuracy                           0.54       102\n",
      "    macro avg       0.61      0.52      0.52       102\n",
      " weighted avg       0.62      0.54      0.54       102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(train_x,train_y)\n",
    "\n",
    "rf_pred = rf_model.predict(test_x)\n",
    "print(classification_report(test_y, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Analytical       0.90      0.82      0.86        11\n",
      "    Confident       1.00      0.89      0.94         9\n",
      "Disrespectful       0.42      0.80      0.55        10\n",
      "     Doubtful       1.00      0.20      0.33        10\n",
      "     Friendly       0.00      0.00      0.00         5\n",
      "      Hostile       0.67      0.50      0.57        12\n",
      "       Joyful       0.75      1.00      0.86         6\n",
      "   Optimistic       0.33      0.56      0.42         9\n",
      "  Pessimistic       0.18      0.25      0.21         8\n",
      "   Respectful       1.00      0.43      0.60         7\n",
      "          Sad       0.77      1.00      0.87        10\n",
      "       Urgent       0.75      0.60      0.67         5\n",
      "\n",
      "     accuracy                           0.61       102\n",
      "    macro avg       0.65      0.59      0.57       102\n",
      " weighted avg       0.67      0.61      0.59       102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "from sklearn import svm\n",
    "\n",
    "svm_model = svm.SVC(kernel='linear')\n",
    "svm_model.fit(train_x,train_y)\n",
    "\n",
    "svm_pred = svm_model.predict(test_x)\n",
    "print(classification_report(test_y, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
